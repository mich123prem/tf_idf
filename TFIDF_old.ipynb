{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TFIDF and cosine similarity - toy example\n",
    "#### Inspired by, and partly taken from the contributions of <a href=\"https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/\">Mark Needham</a>  and <a href=\"https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\">William Scott</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook demonstrate\n",
    "#### Preprocessing of texts for retrieval using the NLTK package\n",
    "\n",
    "#### the use of TFIDF in retrieval. <br>  ~6000 very short documents (stored in the papers/ directory) are read into memory, preprocessed to various degrees and are indexed for retrieval.<br> \n",
    "#### A toy of a toy (10 documents) are available in the directory <b>papers1/</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('papers1/822430.txt', 'Operating System Directions for the Next Millennium')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import glob\n",
    "#\n",
    "corpus = [] # A list of tuples\n",
    "\n",
    "i=0\n",
    "for file in glob.glob(\"papers1/*.txt\"): #\"papers1/*.txt\" - 10 documents ...\n",
    "    with open(file, \"r\") as paper:\n",
    "#        filesfile.write(file[7:-4]+\":  \"+paper.read()+\"\\n\")\n",
    "        corpus.append((file, paper.read()))\n",
    "        i+=1\n",
    "\n",
    "#Define N, the number of documents\n",
    "N=len(corpus)\n",
    "print(corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_split(doc_or_query):\n",
    "    tokens = doc_or_query.lower().split()\n",
    "    processed_doc_or_query = []\n",
    "    for w in tokens:\n",
    "\n",
    "      \n",
    "        if w not in stopwords.words(\"english\"):\n",
    "            processed_doc_or_query.append(w)\n",
    "    return processed_doc_or_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## we introduce preprocessing in two steps that use the nltk-package to different degrees\n",
    "### 1. simple_preprocess() which only uses stop-words and punct. removal\n",
    "### 2. preprocess():             here we can comment in / out different steps, to see the effect\n",
    "### remember to call the correct function both for texts AND queries when experimenting with different preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the simple preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. simple_preprocess:\n",
    "#### HERE WE ONLY IMPORT STOPWORDS LIST FROM NLTK, AND HANDLE PUNCTUATION\n",
    "\n",
    "#### The nltk-package has a lot of useful tools for language technology.<br> \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "# HERE WE USE THE STOPWORDS (NO Stemming, Lemmatization or any other stuff)\n",
    "def simple_preprocess(doc_or_query):\n",
    "    print(\"before:\",doc_or_query)\n",
    "    # returns a list of tokens\n",
    "    txt = doc_or_query\n",
    "\n",
    "    # REMOVE PUNCTUATION\n",
    "    for ch in symbols:\n",
    "        txt = txt.replace(ch, \" \")  # re.sub(string.punctuation, \" \", doc[1])\n",
    "    doc_or_query = txt\n",
    "    print(\"after:\",doc_or_query)\n",
    "    return token_split(txt)\n",
    "    # txt.lower() standardizes to low-case characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the more elaborate preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. preprocess:\n",
    "#### MORE ELABORATE PREPROCESSING WHERE STEPS CAN BE SWITCHED OUT\n",
    "#### BY COMMENTING OUT LINE\n",
    "\n",
    "import preprocess as pp  # We import the python file preprocess.py with preprocessing function\n",
    "\n",
    "\n",
    "def preprocess(doc_or_query):\n",
    "    print(\"before:\",doc_or_query)\n",
    "    doc_or_query = pp.convert_lower_case(doc_or_query)\n",
    "    \n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  # remove comma seperately\n",
    "    \n",
    "    doc_or_query = pp.remove_apostrophe(doc_or_query)\n",
    "    doc_or_query = pp.remove_stop_words(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(doc_or_query)\n",
    "    doc_or_query = pp.remove_punctuation(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as we need to stem the words\n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as num2word is giving few hypens and commas fourty-one\n",
    "    doc_or_query = pp.remove_stop_words(\n",
    "        doc_or_query\n",
    "    )\n",
    "    print(\"after:\",doc_or_query)\n",
    "\n",
    "    return token_split(doc_or_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "### aDF calculated in advance\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "  \n",
    "DF = dict() # dictionary \"Associative Array \"  \n",
    "c=0\n",
    "processed_corpus=[]#An array of token arrays\n",
    "ctr=0\n",
    "for doc in corpus:\n",
    "    processed_text=\"\"\n",
    "    txt=doc[1]\n",
    "    processed_tokens=preprocess(txt)\n",
    "    \n",
    "    #DF includes actually our vocabulary, and for each word its global weight \n",
    "    for w in processed_tokens:\n",
    "        try:\n",
    "            # DF[w] is a set, and each document will only be added once.\n",
    "            DF[w].add(ctr)\n",
    "        except:\n",
    "            DF[w] = {ctr}\n",
    "                \n",
    "    processed_corpus.append(processed_tokens)\n",
    "    ctr += 1\n",
    "print(\"ctr\",ctr)\n",
    "# At the end ctr = N\n",
    "\n",
    "# WE only need the number of distinct documents indexed  by each word.\n",
    "for j in DF:\n",
    "    DF[j]=len(DF[j])\n",
    "\n",
    "    #Print the first token array in processed_corpus\n",
    "print(\"processed_corpus[0]=\",processed_corpus[0])\n",
    "print(\"DF=\", DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COSINE TWO VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the numpy.linalg package to multiply the lengths of the vectors\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b) / ( np.linalg.norm(a) * np.linalg.norm(b) )\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a TD numpy-matrix D, with tfidf-values\n",
    "### For mathematical calculations, it is much better to use the numpy-package.<br> For this we need to reform the tf_idf matrix into a numpy matrix. We call it D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = [x for x in DF]\n",
    "total_vocab_size = len(DF)\n",
    "\n",
    "print()\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "for tpl in tf_idf:  # tpl is a tuple (tpl[0]: document number, tpl[1]: term)\n",
    "    try:\n",
    "        ind = total_vocab.index(tpl[1])\n",
    "        D[tpl[0]][ind] = tf_idf[tpl]   #Put the \n",
    "    except:\n",
    "        print(\"passed\")\n",
    "        pass\n",
    "print(tpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating a vector of tokens \n",
    "##  for example a query vector\n",
    "### This vector can be \"cosined\" with all the document vectors,<br> to get the similarities, and rank by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def gen_vector(tokens):\n",
    "    # We generate a vector of tfidf values the vocabulary from the keys of the DF dictionary\n",
    "    total_vocab = [x for x in DF]\n",
    "    #print(total_vocab)\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        \"\"\"\n",
    "        tf = (\n",
    "            counter[token] / words_count\n",
    "        )  # The local weight: frequency of the token in the vector\n",
    "\n",
    "        # not all query vectors are represented in the vocabulary\n",
    "        if token in DF:\n",
    "            df = DF[token]  # DF is the global weight of the term\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = math.log2((N + 1) / (df + 1))  # log\n",
    "        \"\"\"\n",
    "        tf = counter[token]  # /words_count: occurrences\n",
    "        logtf = 1 + np.log2(tf)  # log\n",
    "        if token in DF:\n",
    "            df = DF[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log2(1 + (N / df))  # log\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = logtf * idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outtable_cos = []\n",
    "\n",
    "\n",
    "def cosine_similarity(query, D=D):\n",
    "    # Create an array of cosine values\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    # tokens = word_tokenize(str(preprocessed_query))\n",
    "    tokens = preprocessed_query\n",
    "    print(\"\\nQuery:\", query)\n",
    "    # print(\"\")\n",
    "    # print(tokens)\n",
    "    # print(\"D=\", type(D))\n",
    "\n",
    "    d_cosines = []\n",
    "\n",
    "    query_vector = gen_vector(tokens)\n",
    "    for q in query_vector:\n",
    "        print(q)\n",
    "    # We go through all vectors in the TD (tfidf) matrix D\n",
    "    # and calculate the cosine for the query against each\n",
    "    for d in D:\n",
    "        cs = cosine_sim(query_vector, d)\n",
    "        if np.isnan(cs):\n",
    "            cs = np.float_(-10e3)\n",
    "        d_cosines.append(cs)\n",
    "\n",
    "    # argsort() returns the indexes that would sort the array.\n",
    "    ## sorts by the cosines, but returns the indexes (document numbers, the first 10.)\n",
    "  \n",
    "    out = np.array(d_cosines).argsort()[-10:][::-1]\n",
    "    outtable_cos.append([\"Query: \", \"'\"+query+\"'\", \"\"])\n",
    "    outtable_cos.append([\"doc_nr\", \"doc\", \"score\"])\n",
    "\n",
    "    for d in out:\n",
    "        outtable_cos.append([d, corpus[d], d_cosines[d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_similarity(\"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")\n",
    "cosine_similarity(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "\n",
    "display(HTML(tabulate.tabulate(outtable_simple, tablefmt=\"html\")))\n",
    "display(HTML(tabulate.tabulate(outtable_cos, tablefmt=\"html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
