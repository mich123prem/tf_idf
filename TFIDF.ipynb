{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TFIDF and cosine similarity - toy example\n",
    "#### Inspired by, and partly taken from the contributions of <a href=\"https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/\">Mark Needham</a>  and <a href=\"https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\">William Scott</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook demonstrate\n",
    "#### Preprocessing of texts for retrieval using the NLTK package\n",
    "\n",
    "#### the use of TFIDF in retrieval. <br>  ~6000 very short documents (stored in the papers/ directory) are read into memory, preprocessed to various degrees and are indexed for retrieval.<br> \n",
    "#### A toy of a toy (10 documents) are available in the directory <b>papers1/</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('papers1/822430.txt', 'Operating System Directions for the Next Millennium')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import glob\n",
    "#\n",
    "corpus = [] # A list of tuples\n",
    "\n",
    "i=0\n",
    "for file in glob.glob(\"papers1/*.txt\"): #\"papers1/*.txt\" - 10 documents ...\n",
    "    with open(file, \"r\") as paper:\n",
    "#        filesfile.write(file[7:-4]+\":  \"+paper.read()+\"\\n\")\n",
    "        corpus.append((file, paper.read()))\n",
    "        i+=1\n",
    "\n",
    "#Define N, the number of documents\n",
    "N=len(corpus)\n",
    "print(corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_split(doc_or_query):\n",
    "    tokens = doc_or_query.lower().split()\n",
    "    processed_doc_or_query = []\n",
    "    for w in tokens:\n",
    "\n",
    "      \n",
    "        if w not in stopwords.words(\"english\"):\n",
    "            processed_doc_or_query.append(w)\n",
    "    return processed_doc_or_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## we introduce preprocessing in two steps that use the nltk-package to different degrees\n",
    "### 1. simple_preprocess() which only uses stop-words and punct. removal\n",
    "### 2. preprocess():             here we can comment in / out different steps, to see the effect\n",
    "### remember to call the correct function both for texts AND queries when experimenting with different preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the simple preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. simple_preprocess:\n",
    "#### HERE WE ONLY IMPORT STOPWORDS LIST FROM NLTK, AND HANDLE PUNCTUATION\n",
    "\n",
    "#### The nltk-package has a lot of useful tools for language technology.<br> \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "# HERE WE USE THE STOPWORDS (NO Stemming, Lemmatization or any other stuff)\n",
    "def simple_preprocess(doc_or_query):\n",
    "    print(\"before:\",doc_or_query)\n",
    "    # returns a list of tokens\n",
    "    txt = doc_or_query\n",
    "\n",
    "    # REMOVE PUNCTUATION\n",
    "    for ch in symbols:\n",
    "        txt = txt.replace(ch, \" \")  # re.sub(string.punctuation, \" \", doc[1])\n",
    "    doc_or_query = txt\n",
    "    print(\"after:\",doc_or_query)\n",
    "    return token_split(txt)\n",
    "    # txt.lower() standardizes to low-case characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the more elaborate preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. preprocess:\n",
    "#### MORE ELABORATE PREPROCESSING WHERE STEPS CAN BE SWITCHED OUT\n",
    "#### BY COMMENTING OUT LINE\n",
    "\n",
    "import preprocess as pp  # We import the python file preprocess.py with preprocessing function\n",
    "\n",
    "\n",
    "def preprocess(doc_or_query):\n",
    "    print(\"before:\",doc_or_query)\n",
    "    doc_or_query = pp.convert_lower_case(doc_or_query)\n",
    "    \n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  # remove comma seperately\n",
    "    \n",
    "    doc_or_query = pp.remove_apostrophe(doc_or_query)\n",
    "    doc_or_query = pp.remove_stop_words(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(doc_or_query)\n",
    "    doc_or_query = pp.remove_punctuation(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as we need to stem the words\n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as num2word is giving few hypens and commas fourty-one\n",
    "    doc_or_query = pp.remove_stop_words(\n",
    "        doc_or_query\n",
    "    )\n",
    "    print(\"after:\",doc_or_query)\n",
    "\n",
    "    return token_split(doc_or_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Operating System Directions for the Next Millennium\n",
      "after:  oper system direct next millennium\n",
      "before: Operating System Concepts, 4th Ed.\n",
      "after:  oper system concept 4th ed\n",
      "before: On attaining reliable software for a secure operating system\n",
      "after:  attain reliabl softwar secur oper system\n",
      "before: Removing backing store administration from the CAP operating system\n",
      "after:  remov back store administr cap oper system\n",
      "before: Reflective program generation with patterns\n",
      "after:  reflect program gener pattern\n",
      "before: Can We Make Operating Systems Reliable and Secure?\n",
      "after:  make oper system reliabl secur\n",
      "before: Designing a global name service\n",
      "after:  design global name servic\n",
      "before: Adaptive feedback techniques for synchronized multimedia retrieval over integrated networks\n",
      "after:  adapt feedback techniqu synchron multimedia retriev integr network\n",
      "before: A hierarchical fair service curve algorithm for link-sharing, real-time and priority services\n",
      "after:  hierarch fair servic curv algorithm link share real time prioriti servic\n",
      "before: The Chubby lock service for loosely-coupled distributed systems\n",
      "after:  chubbi lock servic loo coupl distribut system\n",
      "ctr 10\n",
      "processed_corpus[0]= ['oper', 'system', 'direct', 'next', 'millennium']\n",
      "DF= {'oper': 5, 'system': 6, 'direct': 1, 'next': 1, 'millennium': 1, 'concept': 1, '4th': 1, 'ed': 1, 'attain': 1, 'reliabl': 2, 'softwar': 1, 'secur': 2, 'remov': 1, 'back': 1, 'store': 1, 'administr': 1, 'cap': 1, 'reflect': 1, 'program': 1, 'gener': 1, 'pattern': 1, 'make': 1, 'design': 1, 'global': 1, 'name': 1, 'servic': 3, 'adapt': 1, 'feedback': 1, 'techniqu': 1, 'synchron': 1, 'multimedia': 1, 'retriev': 1, 'integr': 1, 'network': 1, 'hierarch': 1, 'fair': 1, 'curv': 1, 'algorithm': 1, 'link': 1, 'share': 1, 'real': 1, 'time': 1, 'prioriti': 1, 'chubbi': 1, 'lock': 1, 'loo': 1, 'coupl': 1, 'distribut': 1}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "### aDF calculated in advance\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "  \n",
    "DF = dict() # dictionary \"Associative Array \"  \n",
    "c=0\n",
    "processed_corpus=[]#An array of token arrays\n",
    "ctr=0\n",
    "for doc in corpus:\n",
    "    processed_text=\"\"\n",
    "    txt=doc[1]\n",
    "    processed_tokens=preprocess(txt)\n",
    "    \n",
    "    #DF includes actually our vocabulary, and for each word its global weight \n",
    "    for w in processed_tokens:\n",
    "        try:\n",
    "            # DF[w] is a set, and each document will only be added once.\n",
    "            DF[w].add(ctr)\n",
    "        except:\n",
    "            DF[w] = {ctr}\n",
    "                \n",
    "    processed_corpus.append(processed_tokens)\n",
    "    ctr += 1\n",
    "print(\"ctr\",ctr)\n",
    "# At the end ctr = N\n",
    "\n",
    "# We only need the number of distinct documents indexed  by each word.\n",
    "for j in DF:\n",
    "    DF[j]=len(DF[j])\n",
    "\n",
    "    #Print the first token array in processed_corpus\n",
    "print(\"processed_corpus[0]=\",processed_corpus[0])\n",
    "print(\"DF=\", DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Define a function that calculates the tfidf value of a term (token) in a document based on how many times the term occurs in the document\n",
    "## Use the function \n",
    "### To represent documents (before retrieval)\n",
    "### To represent a query (later on, at retrieval time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tfidf(count, token):\n",
    "    #\n",
    "    #== CAN CHANGE THE DEFINITION OF TF ==\n",
    "    #==     SEE Table 3.4 in book       == \n",
    "    #\n",
    "    tf=count\n",
    "    tf = 1 + np.log2(tf)  # log\n",
    "    if token in DF:\n",
    "        df = DF[token]\n",
    "        #\n",
    "        #== CAN CHANGE THE DEFINITION OF IDF ==\n",
    "        #==     SEE Table 3.5 in book       == \n",
    "        #\n",
    "        #idf=1/df\n",
    "        idf = np.log2(((N)/ df))  # log\n",
    "\n",
    "        tf_idf = (\n",
    "            tf * idf\n",
    "        )  \n",
    "    else: #Eq. 3.7\n",
    "        tf_idf=0\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a representation of each token in each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'direct'): 3.321928094887362,\n",
       " (0, 'millennium'): 3.321928094887362,\n",
       " (0, 'next'): 3.321928094887362,\n",
       " (0, 'oper'): 1.0,\n",
       " (0, 'system'): 0.7369655941662062,\n",
       " (1, '4th'): 3.321928094887362,\n",
       " (1, 'concept'): 3.321928094887362,\n",
       " (1, 'ed'): 3.321928094887362,\n",
       " (1, 'oper'): 1.0,\n",
       " (1, 'system'): 0.7369655941662062,\n",
       " (2, 'attain'): 3.321928094887362,\n",
       " (2, 'oper'): 1.0,\n",
       " (2, 'reliabl'): 2.321928094887362,\n",
       " (2, 'secur'): 2.321928094887362,\n",
       " (2, 'softwar'): 3.321928094887362,\n",
       " (2, 'system'): 0.7369655941662062,\n",
       " (3, 'administr'): 3.321928094887362,\n",
       " (3, 'back'): 3.321928094887362,\n",
       " (3, 'cap'): 3.321928094887362,\n",
       " (3, 'oper'): 1.0,\n",
       " (3, 'remov'): 3.321928094887362,\n",
       " (3, 'store'): 3.321928094887362,\n",
       " (3, 'system'): 0.7369655941662062,\n",
       " (4, 'gener'): 3.321928094887362,\n",
       " (4, 'pattern'): 3.321928094887362,\n",
       " (4, 'program'): 3.321928094887362,\n",
       " (4, 'reflect'): 3.321928094887362,\n",
       " (5, 'make'): 3.321928094887362,\n",
       " (5, 'oper'): 1.0,\n",
       " (5, 'reliabl'): 2.321928094887362,\n",
       " (5, 'secur'): 2.321928094887362,\n",
       " (5, 'system'): 0.7369655941662062,\n",
       " (6, 'design'): 3.321928094887362,\n",
       " (6, 'global'): 3.321928094887362,\n",
       " (6, 'name'): 3.321928094887362,\n",
       " (6, 'servic'): 1.736965594166206,\n",
       " (7, 'adapt'): 3.321928094887362,\n",
       " (7, 'feedback'): 3.321928094887362,\n",
       " (7, 'integr'): 3.321928094887362,\n",
       " (7, 'multimedia'): 3.321928094887362,\n",
       " (7, 'network'): 3.321928094887362,\n",
       " (7, 'retriev'): 3.321928094887362,\n",
       " (7, 'synchron'): 3.321928094887362,\n",
       " (7, 'techniqu'): 3.321928094887362,\n",
       " (8, 'algorithm'): 3.321928094887362,\n",
       " (8, 'curv'): 3.321928094887362,\n",
       " (8, 'fair'): 3.321928094887362,\n",
       " (8, 'hierarch'): 3.321928094887362,\n",
       " (8, 'link'): 3.321928094887362,\n",
       " (8, 'prioriti'): 3.321928094887362,\n",
       " (8, 'real'): 3.321928094887362,\n",
       " (8, 'servic'): 3.473931188332412,\n",
       " (8, 'share'): 3.321928094887362,\n",
       " (8, 'time'): 3.321928094887362,\n",
       " (9, 'chubbi'): 3.321928094887362,\n",
       " (9, 'coupl'): 3.321928094887362,\n",
       " (9, 'distribut'): 3.321928094887362,\n",
       " (9, 'lock'): 3.321928094887362,\n",
       " (9, 'loo'): 3.321928094887362,\n",
       " (9, 'servic'): 1.736965594166206,\n",
       " (9, 'system'): 0.7369655941662062}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "doc = 0\n",
    "tf_idf = {}  # Initializing the representation\n",
    "for doc in range(N):  # For all documents\n",
    "    tokens = processed_corpus[doc]\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "    #How often does each token occur in the document    \n",
    "    counts = Counter(tokens)  # counts unique tokens in the tokens list\n",
    "    for token in np.unique(tokens):  # sorted unique tokens\n",
    "        tf_idf[doc, token]=calc_tfidf(counts[token], token)\n",
    "\n",
    "# Printing an example from the Matrix: the TFIDF value of the word \"Automated\" in document 1\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a TD numpy-matrix D, with tfidf-values\n",
    "### For mathematical calculations, it is much better to use the numpy-package.<br> For this we need to reform the tf_idf matrix into a numpy matrix. We call it TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.         0.73696559 3.32192809 3.32192809 3.32192809 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_vocab = [x for x in DF]\n",
    "total_vocab_size = len(DF)\n",
    "\n",
    "print()\n",
    "TD = np.zeros((N, total_vocab_size)) \n",
    "for doc_nr, token in tf_idf:  \n",
    "    ind = total_vocab.index(token)\n",
    "    TD[doc_nr][ind] = tf_idf[doc_nr, token]   #Put the \n",
    "   \n",
    "print(TD[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating a vector of tokens \n",
    "## to be used for preparing a query vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def gen_vector(tokens):\n",
    "    # We generate a vector of tfidf values the vocabulary from the keys of the DF dictionary\n",
    "    total_vocab = [x for x in DF]\n",
    "    #print(total_vocab)\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "\n",
    "    tf_s = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        ind = total_vocab.index(token)\n",
    "        Q[ind] = calc_tfidf(tf_s[token], token)\n",
    "        \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Such a query vector can be \"cosined\" with all the document vectors,<br> \n",
    "### to get the similarities, and rank by them. We define cosine(query, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the numpy.linalg package to multiply the lengths of the vectors\n",
    "def cosine(q, d):\n",
    "    lengths = np.linalg.norm(q) * np.linalg.norm(d)\n",
    "    dotproduct = np.dot(q, d)\n",
    "    cos_sim = dotproduct / lengths\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This function calculates the similarity between a query and each document, and generate a ranked list  (most_similar .... least_similar). See example of usage below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outtable_cos = []\n",
    "\n",
    "\n",
    "def cosine_similarity(query, D=TD):\n",
    "    # Create an array of cosine values\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    # tokens = word_tokenize(str(preprocessed_query))\n",
    "    tokens = preprocessed_query\n",
    "    print(\"\\nQuery:\", query)\n",
    "    # print(\"\")\n",
    "    # print(tokens)\n",
    "    # print(\"D=\", type(D))\n",
    "\n",
    "    d_cosines = []\n",
    "\n",
    "    query_vector = gen_vector(tokens)\n",
    "    for q in query_vector:\n",
    "        print(q)\n",
    "    # We go through all vectors in the TD (tfidf) matrix D\n",
    "    # and calculate the cosine for the query against each\n",
    "    for d in D:\n",
    "        cs = cosine(query_vector, d)\n",
    "        if np.isnan(cs):\n",
    "            cs = np.float_(-10e3)\n",
    "        d_cosines.append(cs)\n",
    "\n",
    "    # argsort() returns the indexes that would sort the array.\n",
    "    ## sorts by the cosines, but returns the indexes (document numbers, the first 10.)\n",
    "  \n",
    "    out = np.array(d_cosines).argsort()[-10:][::-1]\n",
    "    outtable_cos.append([\"Query: \", \"'\"+query+\"'\", \"\"])\n",
    "    outtable_cos.append([\"doc_nr\", \"doc\", \"score\"])\n",
    "\n",
    "    for d in out:\n",
    "        outtable_cos.append([d, corpus[d], d_cosines[d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "before: make Operating system\n",
      "after:  make oper system\n",
      "\n",
      "Query: make Operating system\n",
      "1.0\n",
      "0.7369655941662062\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.321928094887362\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity(\"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")\n",
    "query=\"make Operating system\"\n",
    "cosine_similarity(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Query:</td><td>'make Operating system'                                                                                                </td><td>                   </td></tr>\n",
       "<tr><td>doc_nr</td><td>doc                                                                                                                    </td><td>score              </td></tr>\n",
       "<tr><td>5     </td><td>('papers1/1137291.txt', 'Can We Make Operating Systems Reliable and Secure?')                                          </td><td>0.7337792962152588 </td></tr>\n",
       "<tr><td>2     </td><td>('papers1/808449.txt', 'On attaining reliable software for a secure operating system')                                 </td><td>0.0741877827280753 </td></tr>\n",
       "<tr><td>1     </td><td>('papers1/562353.txt', 'Operating System Concepts, 4th Ed.')                                                           </td><td>0.07391696300292849</td></tr>\n",
       "<tr><td>0     </td><td>('papers1/822430.txt', 'Operating System Directions for the Next Millennium')                                          </td><td>0.07391696300292849</td></tr>\n",
       "<tr><td>3     </td><td>('papers1/850712.txt', 'Removing backing store administration from the CAP operating system')                          </td><td>0.05777273984478571</td></tr>\n",
       "<tr><td>9     </td><td>('papers1/1298487.txt', 'The Chubby lock service for loosely-coupled distributed systems')                             </td><td>0.01998159296741262</td></tr>\n",
       "<tr><td>8     </td><td>('papers1/263175.txt', 'A hierarchical fair service curve algorithm for link-sharing, real-time and priority services')</td><td>0.0                </td></tr>\n",
       "<tr><td>7     </td><td>('papers1/153386.txt', 'Adaptive feedback techniques for synchronized multimedia retrieval over integrated networks')  </td><td>0.0                </td></tr>\n",
       "<tr><td>6     </td><td>('papers1/10591.txt', 'Designing a global name service')                                                               </td><td>0.0                </td></tr>\n",
       "<tr><td>4     </td><td>('papers1/1173748.txt', 'Reflective program generation with patterns')                                                 </td><td>0.0                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "\n",
    "#display(HTML(tabulate.tabulate(outtable_simple, tablefmt=\"html\")))\n",
    "display(HTML(tabulate.tabulate(outtable_cos, tablefmt=\"html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
