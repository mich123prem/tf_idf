{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TFIDF and cosine similarity - toy example\n",
    "#### Inspired by, and partly taken from the contributions of <a href=\"https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/\">Mark Needham</a>  and <a href=\"https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\">William Scott</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook demonstrate the use of TFIDF in retrieval. <br> ~6000 very short documents (stored in the papers/ directory) are read into memory, preprocessed to various degrees and are indexed for retrieval.<br> \n",
    "#### A toy of a toy (10 documents) are available in the directory <b>papers1/</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('papers1/822430.txt', 'Operating System Directions for the Next Millennium')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import glob\n",
    "#\n",
    "corpus = [] # A list of tuples\n",
    "\n",
    "i=0\n",
    "for file in glob.glob(\"papers1/*.txt\"): #\"papers1/*.txt\" - 10 documents ...\n",
    "    with open(file, \"r\") as paper:\n",
    "#        filesfile.write(file[7:-4]+\":  \"+paper.read()+\"\\n\")\n",
    "        corpus.append((file, paper.read()))\n",
    "        i+=1\n",
    "\n",
    "#Define N, the number of documents\n",
    "N=len(corpus)\n",
    "print(corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_split(doc_or_query):\n",
    "    tokens = doc_or_query.lower().split()\n",
    "    processed_doc_or_query = []\n",
    "    for w in tokens:\n",
    "\n",
    "      \n",
    "        if w not in stopwords.words(\"english\"):\n",
    "            processed_doc_or_query.append(w)\n",
    "    return processed_doc_or_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## we introduce preprocessing in two steps that use the nltk-package to different degrees\n",
    "### 1. simple_preprocess() which only uses stop-words and punct. removal\n",
    "### 2. preprocess():             here we can comment in / out different steps, to see the effect\n",
    "### remember to call the correct function both for texts AND queries when experimenting with different preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the simple preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. simple_preprocess:\n",
    "#### HERE WE ONLY IMPORT STOPWORDS LIST FROM NLTK, AND HANDLE PUNCTUATION\n",
    "\n",
    "#### The nltk-package has a lot of useful tools for language technology.<br> \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "# HERE WE USE THE STOPWORDS (NO Stemming, Lemmatization or any other stuff)\n",
    "def simple_preprocess(doc_or_query):\n",
    "    # returns a list of tokens\n",
    "    txt = doc_or_query\n",
    "\n",
    "    # REMOVE PUNCTUATION\n",
    "    for ch in symbols:\n",
    "        txt = txt.replace(ch, \" \")  # re.sub(string.punctuation, \" \", doc[1])\n",
    "    return token_split(txt)\n",
    "    # txt.lower() standardizes to low-case characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the more elaborate preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. preprocess:\n",
    "#### MORE ELABORATE PREPROCESSING WHERE STEPS CAN BE SWITCHED OUT\n",
    "#### BY COMMENTING OUT LINE\n",
    "\n",
    "import preprocess as pp  # We import the python file preprocess.py with preprocessing function\n",
    "\n",
    "\n",
    "def preprocess(doc_or_query):\n",
    "    print(\"before:\",doc_or_query)\n",
    "    doc_or_query = pp.convert_lower_case(doc_or_query)\n",
    "    \n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  # remove comma seperately\n",
    "    \n",
    "    doc_or_query = pp.remove_apostrophe(doc_or_query)\n",
    "    doc_or_query = pp.remove_stop_words(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(doc_or_query)\n",
    "    doc_or_query = pp.remove_punctuation(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as we need to stem the words\n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as num2word is giving few hypens and commas fourty-one\n",
    "    doc_or_query = pp.remove_stop_words(\n",
    "        doc_or_query\n",
    "    )\n",
    "    print(\"after:\",doc_or_query)\n",
    "\n",
    "    return token_split(doc_or_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Operating System Directions for the Next Millennium\n",
      "after:  oper system direct next millennium\n",
      "before: Operating System Concepts, 4th Ed.\n",
      "after:  oper system concept 4th ed\n",
      "before: On attaining reliable software for a secure operating system\n",
      "after:  attain reliabl softwar secur oper system\n",
      "before: Removing backing store administration from the CAP operating system\n",
      "after:  remov back store administr cap oper system\n",
      "before: Reflective program generation with patterns\n",
      "after:  reflect program gener pattern\n",
      "before: Can We Make Operating Systems Reliable and Secure?\n",
      "after:  make oper system reliabl secur\n",
      "before: Designing a global name service\n",
      "after:  design global name servic\n",
      "before: Adaptive feedback techniques for synchronized multimedia retrieval over integrated networks\n",
      "after:  adapt feedback techniqu synchron multimedia retriev integr network\n",
      "before: A hierarchical fair service curve algorithm for link-sharing, real-time and priority services\n",
      "after:  hierarch fair servic curv algorithm link share real time prioriti servic\n",
      "before: The Chubby lock service for loosely-coupled distributed systems\n",
      "after:  chubbi lock servic loo coupl distribut system\n",
      "ctr 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'oper': 5,\n",
       " 'system': 6,\n",
       " 'direct': 1,\n",
       " 'next': 1,\n",
       " 'millennium': 1,\n",
       " 'concept': 1,\n",
       " '4th': 1,\n",
       " 'ed': 1,\n",
       " 'attain': 1,\n",
       " 'reliabl': 2,\n",
       " 'softwar': 1,\n",
       " 'secur': 2,\n",
       " 'remov': 1,\n",
       " 'back': 1,\n",
       " 'store': 1,\n",
       " 'administr': 1,\n",
       " 'cap': 1,\n",
       " 'reflect': 1,\n",
       " 'program': 1,\n",
       " 'gener': 1,\n",
       " 'pattern': 1,\n",
       " 'make': 1,\n",
       " 'design': 1,\n",
       " 'global': 1,\n",
       " 'name': 1,\n",
       " 'servic': 3,\n",
       " 'adapt': 1,\n",
       " 'feedback': 1,\n",
       " 'techniqu': 1,\n",
       " 'synchron': 1,\n",
       " 'multimedia': 1,\n",
       " 'retriev': 1,\n",
       " 'integr': 1,\n",
       " 'network': 1,\n",
       " 'hierarch': 1,\n",
       " 'fair': 1,\n",
       " 'curv': 1,\n",
       " 'algorithm': 1,\n",
       " 'link': 1,\n",
       " 'share': 1,\n",
       " 'real': 1,\n",
       " 'time': 1,\n",
       " 'prioriti': 1,\n",
       " 'chubbi': 1,\n",
       " 'lock': 1,\n",
       " 'loo': 1,\n",
       " 'coupl': 1,\n",
       " 'distribut': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "### aDF calculated in advance\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "  \n",
    "DF = {}\n",
    "c=0\n",
    "processed_corpus=[]#An array of token arrays\n",
    "ctr=0\n",
    "for doc in corpus:\n",
    "    processed_text=\"\"\n",
    "    txt=doc[1]\n",
    "    processed_tokens=preprocess(txt)\n",
    "    \n",
    "    #DF includes actually our vocabulary, and for each word its global weight \n",
    "    for w in processed_tokens:\n",
    "        try:\n",
    "            # DF[w] is a set, and each document will only be added once.\n",
    "            DF[w].add(ctr)\n",
    "        except:\n",
    "            DF[w] = {ctr}\n",
    "                \n",
    "    processed_corpus.append(processed_tokens)\n",
    "    ctr += 1\n",
    "print(\"ctr\",ctr)\n",
    "# At the end ctr = N\n",
    "\n",
    "# WE only need the number of distinct documents indexed  by each word.\n",
    "for j in DF:\n",
    "    DF[j]=len(DF[j])\n",
    "\n",
    "    #Print the first token array in processed_corpus\n",
    "processed_corpus[0]\n",
    "DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Form the TFIDF valued TD-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 'direct'): 3.4594316186372973, (0, 'millennium'): 3.4594316186372973, (0, 'next'): 3.4594316186372973, (0, 'oper'): 1.584962500721156, (0, 'system'): 1.415037499278844, (1, '4th'): 3.4594316186372973, (1, 'concept'): 3.4594316186372973, (1, 'ed'): 3.4594316186372973, (1, 'oper'): 1.584962500721156, (1, 'system'): 1.415037499278844, (2, 'attain'): 3.4594316186372973, (2, 'oper'): 1.584962500721156, (2, 'reliabl'): 2.584962500721156, (2, 'secur'): 2.584962500721156, (2, 'softwar'): 3.4594316186372973, (2, 'system'): 1.415037499278844, (3, 'administr'): 3.4594316186372973, (3, 'back'): 3.4594316186372973, (3, 'cap'): 3.4594316186372973, (3, 'oper'): 1.584962500721156, (3, 'remov'): 3.4594316186372973, (3, 'store'): 3.4594316186372973, (3, 'system'): 1.415037499278844, (4, 'gener'): 3.4594316186372973, (4, 'pattern'): 3.4594316186372973, (4, 'program'): 3.4594316186372973, (4, 'reflect'): 3.4594316186372973, (5, 'make'): 3.4594316186372973, (5, 'oper'): 1.584962500721156, (5, 'reliabl'): 2.584962500721156, (5, 'secur'): 2.584962500721156, (5, 'system'): 1.415037499278844, (6, 'design'): 3.4594316186372973, (6, 'global'): 3.4594316186372973, (6, 'name'): 3.4594316186372973, (6, 'servic'): 2.115477217419936, (7, 'adapt'): 3.4594316186372973, (7, 'feedback'): 3.4594316186372973, (7, 'integr'): 3.4594316186372973, (7, 'multimedia'): 3.4594316186372973, (7, 'network'): 3.4594316186372973, (7, 'retriev'): 3.4594316186372973, (7, 'synchron'): 3.4594316186372973, (7, 'techniqu'): 3.4594316186372973, (8, 'algorithm'): 2.0236377707119106, (8, 'curv'): 2.0236377707119106, (8, 'fair'): 2.0236377707119106, (8, 'hierarch'): 2.0236377707119106, (8, 'link'): 2.0236377707119106, (8, 'prioriti'): 2.0236377707119106, (8, 'real'): 2.0236377707119106, (8, 'servic'): 2.115477217419936, (8, 'share'): 2.0236377707119106, (8, 'time'): 2.0236377707119106, (9, 'chubbi'): 3.4594316186372973, (9, 'coupl'): 3.4594316186372973, (9, 'distribut'): 3.4594316186372973, (9, 'lock'): 3.4594316186372973, (9, 'loo'): 3.4594316186372973, (9, 'servic'): 2.115477217419936, (9, 'system'): 1.415037499278844}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "doc = 0\n",
    "tf_idf = {}  # Initializing the matrix\n",
    "showed = {}\n",
    "for d in range(N):  # For all documents\n",
    "    tokens = processed_corpus[d]\n",
    "    counter = Counter(tokens)  # counts unique tokens in the tokens array\n",
    "    # and creates a dictionary of tokens counts\n",
    "    #print(\"counter\", counter)\n",
    "    maxi_fij=counter.most_common(1)[0][1]\n",
    "    #print(\"max\",maxi_fij, type(maxi_fij))\n",
    "    \n",
    "    words_count = len(tokens)\n",
    "    for token in np.unique(tokens):  # sorted unique tokens\n",
    "        tf = 0.5 + 0.5*(counter[token]/maxi_fij)# counter[token]  # /words_count\n",
    "        logtf = 1 + np.log2(tf)  # log\n",
    "        if token in DF:\n",
    "            df = DF[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log2(1 + (N / df))  # log\n",
    "\n",
    "        tf_idf[d, token] = (\n",
    "            logtf * idf\n",
    "        )  # tf_idf is implemented as tuple-keyed dictionary\n",
    "doc += 1\n",
    "# Printing an example from the Matrix: the TFIDF value of the word \"Automated\" in document 1\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple matching of a query   \n",
    "## add up term - TFIDF scores for each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "outtable_simple = []\n",
    "\n",
    "\n",
    "def matching_score(query):\n",
    "    # Process the query just like you processed the documents\n",
    "    # remove punctuation\n",
    "    processed_tokens = preprocess(query)\n",
    "\n",
    "    tokens = processed_tokens\n",
    "    print(\"Matching Score\")\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"tokens:\")\n",
    "    print(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    # Simply add up the tfidfs for the words in the documents they index.\n",
    "    for key in tf_idf:\n",
    "        # remember, key is composed of\n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                print(\"key[1]\", key[1])\n",
    "                if key[0] not in query_weights:\n",
    "                    query_weights[key[0]]=0.0\n",
    "                query_weights[key[0]] += tf_idf[key]  # Accummulate and add tfidf-values for the term in each document\n",
    "            except:\n",
    "                print(\"exception:\", key, tf_idf[key])\n",
    "                query_weights[key[0]] = tf_idf[key]  #\n",
    "            # print(\"query_weights[\"+str(key[0])+\"]=\"+str(query_weights[key[0]]))\n",
    "    # Sort the resulting weights to give us a ranked list\n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # print(\"query_weights\", query_weights)\n",
    "\n",
    "    l = []\n",
    "    qw = []\n",
    "    # List the first 10 matching documents\n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "        qw.append(i[1])\n",
    "    queue = deque(qw)\n",
    "    outtable_simple.append([\"Query: \", \"'\"+query+\"'\", \"\"])\n",
    "    outtable_simple.append([\"doc_nr\", \"doc\", \"score\"])\n",
    "    for d in l:\n",
    "        score = queue.popleft()\n",
    "        outtable_simple.append([d, corpus[d], score])\n",
    "        # print(d,corpus[d], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Operating system\n",
      "after:  oper system\n",
      "Matching Score\n",
      "\n",
      "Query: Operating system\n",
      "tokens:\n",
      "['oper', 'system']\n",
      "key[1] oper\n",
      "key[1] system\n",
      "key[1] oper\n",
      "key[1] system\n",
      "key[1] oper\n",
      "key[1] system\n",
      "key[1] oper\n",
      "key[1] system\n",
      "key[1] oper\n",
      "key[1] system\n",
      "key[1] system\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Query:</td><td>'Operating system'                                                                           </td><td>                 </td></tr>\n",
       "<tr><td>doc_nr</td><td>doc                                                                                          </td><td>score            </td></tr>\n",
       "<tr><td>0     </td><td>('papers1/822430.txt', 'Operating System Directions for the Next Millennium')                </td><td>3.0              </td></tr>\n",
       "<tr><td>1     </td><td>('papers1/562353.txt', 'Operating System Concepts, 4th Ed.')                                 </td><td>3.0              </td></tr>\n",
       "<tr><td>2     </td><td>('papers1/808449.txt', 'On attaining reliable software for a secure operating system')       </td><td>3.0              </td></tr>\n",
       "<tr><td>3     </td><td>('papers1/850712.txt', 'Removing backing store administration from the CAP operating system')</td><td>3.0              </td></tr>\n",
       "<tr><td>5     </td><td>('papers1/1137291.txt', 'Can We Make Operating Systems Reliable and Secure?')                </td><td>3.0              </td></tr>\n",
       "<tr><td>9     </td><td>('papers1/1298487.txt', 'The Chubby lock service for loosely-coupled distributed systems')   </td><td>1.415037499278844</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#matching_score(\"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")\n",
    "query=\"Operating system\"\n",
    "matching_score(query)\n",
    "\n",
    "display(HTML(tabulate.tabulate(outtable_simple, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the numpy.linalg package to multiply the lengths of the vectors\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating a TD numpy-matrix D, with tfidf-values\n",
    "### For mathematical calculations, it is much better to use the numpy-package.<br> For this we need to reform the tf_idf matrix into a numpy matrix. We call it D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(9, 'system')\n"
     ]
    }
   ],
   "source": [
    "total_vocab = [x for x in DF]\n",
    "total_vocab_size = len(DF)\n",
    "\n",
    "print()\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "for tpl in tf_idf:  # tpl is a tuple (tpl[0]: document number tpl[1] term)\n",
    "    try:\n",
    "        ind = total_vocab.index(tpl[1])\n",
    "        D[tpl[0]][ind] = tf_idf[tpl]\n",
    "    except:\n",
    "        print(\"passed\")\n",
    "        pass\n",
    "print(tpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generating a vector of tokens \n",
    "##  for example a query vector\n",
    "### This vector can be \"cosined\" with all the document vectors,<br> to get the similarities, and rank by them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def gen_vector(tokens):\n",
    "    # We generate a vector of tfidf values the vocabulary from the keys of the DF dictionary\n",
    "    total_vocab = [x for x in DF]\n",
    "    print(total_vocab)\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "\n",
    "    for token in np.unique(tokens):\n",
    "        \"\"\"\n",
    "        tf = (\n",
    "            counter[token] / words_count\n",
    "        )  # The local weight: frequency of the token in the vector\n",
    "\n",
    "        # not all query vectors are represented in the vocabulary\n",
    "        if token in DF:\n",
    "            df = DF[token]  # DF is the global weight of the term\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = math.log2((N + 1) / (df + 1))  # log\n",
    "        \"\"\"\n",
    "        tf = counter[token]  # /words_count\n",
    "        logtf = 1 + np.log2(tf)  # log\n",
    "        if token in DF:\n",
    "            df = DF[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log2(1 + (N / df))  # log\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf * idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "outtable_cos = []\n",
    "\n",
    "\n",
    "def cosine_similarity(query, D=D):\n",
    "    # Create an array of cosine values\n",
    "    print(\"Cosine Similarity\")\n",
    "    preprocessed_query = preprocess(query)\n",
    "    # tokens = word_tokenize(str(preprocessed_query))\n",
    "    tokens = preprocessed_query\n",
    "    print(\"\\nQuery:\", query)\n",
    "    print(\"\")\n",
    "    print(tokens)\n",
    "    # print(\"D=\", type(D))\n",
    "\n",
    "    d_cosines = []\n",
    "\n",
    "    query_vector = gen_vector(tokens)\n",
    "    for q in query_vector:\n",
    "        print(q)\n",
    "    # We go through all vectors in the TD (tfidf) matrix D\n",
    "    for d in D:\n",
    "        cs = cosine_sim(query_vector, d)\n",
    "        if np.isnan(cs):\n",
    "            cs = np.float_(-10e3)\n",
    "        d_cosines.append(cs)\n",
    "\n",
    "    # argsort() returns the indexes that would sort the array.\n",
    "    ## sorts by the cosines, but returns the indexes (document numbers, the first 10.)\n",
    "  \n",
    "    out = np.array(d_cosines).argsort()[-10:][::-1]\n",
    "    outtable_cos.append([\"Query: \", \"'\"+query+\"'\", \"\"])\n",
    "    outtable_cos.append([\"doc_nr\", \"doc\", \"score\"])\n",
    "\n",
    "    for d in out:\n",
    "        outtable_cos.append([d, corpus[d], d_cosines[d]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n",
      "before: Operating system\n",
      "after:  oper system\n",
      "\n",
      "Query: Operating system\n",
      "\n",
      "['oper', 'system']\n",
      "['oper', 'system', 'direct', 'next', 'millennium', 'concept', '4th', 'ed', 'attain', 'reliabl', 'softwar', 'secur', 'remov', 'back', 'store', 'administr', 'cap', 'reflect', 'program', 'gener', 'pattern', 'make', 'design', 'global', 'name', 'servic', 'adapt', 'feedback', 'techniqu', 'synchron', 'multimedia', 'retriev', 'integr', 'network', 'hierarch', 'fair', 'curv', 'algorithm', 'link', 'share', 'real', 'time', 'prioriti', 'chubbi', 'lock', 'loo', 'coupl', 'distribut']\n",
      "1.584962500721156\n",
      "1.415037499278844\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# cosine_similarity(\"Without the drive of Rebeccah's insistence, Kate lost her momentum. She stood next a slatted oak bench, canisters still clutched, surveying\")\n",
    "cosine_similarity(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Query:</td><td>'Operating system'                                                                           </td><td>                 </td></tr>\n",
       "<tr><td>doc_nr</td><td>doc                                                                                          </td><td>score            </td></tr>\n",
       "<tr><td>0     </td><td>('papers1/822430.txt', 'Operating System Directions for the Next Millennium')                </td><td>3.0              </td></tr>\n",
       "<tr><td>1     </td><td>('papers1/562353.txt', 'Operating System Concepts, 4th Ed.')                                 </td><td>3.0              </td></tr>\n",
       "<tr><td>2     </td><td>('papers1/808449.txt', 'On attaining reliable software for a secure operating system')       </td><td>3.0              </td></tr>\n",
       "<tr><td>3     </td><td>('papers1/850712.txt', 'Removing backing store administration from the CAP operating system')</td><td>3.0              </td></tr>\n",
       "<tr><td>5     </td><td>('papers1/1137291.txt', 'Can We Make Operating Systems Reliable and Secure?')                </td><td>3.0              </td></tr>\n",
       "<tr><td>9     </td><td>('papers1/1298487.txt', 'The Chubby lock service for loosely-coupled distributed systems')   </td><td>1.415037499278844</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Query:</td><td>'Operating system'                                                                                                     </td><td>                   </td></tr>\n",
       "<tr><td>doc_nr</td><td>doc                                                                                                                    </td><td>score              </td></tr>\n",
       "<tr><td>5     </td><td>('papers1/1137291.txt', 'Can We Make Operating Systems Reliable and Secure?')                                          </td><td>0.38891754067954415</td></tr>\n",
       "<tr><td>1     </td><td>('papers1/562353.txt', 'Operating System Concepts, 4th Ed.')                                                           </td><td>0.3342084427272912 </td></tr>\n",
       "<tr><td>0     </td><td>('papers1/822430.txt', 'Operating System Directions for the Next Millennium')                                          </td><td>0.3342084427272912 </td></tr>\n",
       "<tr><td>2     </td><td>('papers1/808449.txt', 'On attaining reliable software for a secure operating system')                                 </td><td>0.32858052288449985</td></tr>\n",
       "<tr><td>3     </td><td>('papers1/850712.txt', 'Removing backing store administration from the CAP operating system')                          </td><td>0.26486109984002254</td></tr>\n",
       "<tr><td>9     </td><td>('papers1/1298487.txt', 'The Chubby lock service for loosely-coupled distributed systems')                             </td><td>0.11572447490655624</td></tr>\n",
       "<tr><td>8     </td><td>('papers1/263175.txt', 'A hierarchical fair service curve algorithm for link-sharing, real-time and priority services')</td><td>0.0                </td></tr>\n",
       "<tr><td>7     </td><td>('papers1/153386.txt', 'Adaptive feedback techniques for synchronized multimedia retrieval over integrated networks')  </td><td>0.0                </td></tr>\n",
       "<tr><td>6     </td><td>('papers1/10591.txt', 'Designing a global name service')                                                               </td><td>0.0                </td></tr>\n",
       "<tr><td>4     </td><td>('papers1/1173748.txt', 'Reflective program generation with patterns')                                                 </td><td>0.0                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "\n",
    "display(HTML(tabulate.tabulate(outtable_simple, tablefmt=\"html\")))\n",
    "display(HTML(tabulate.tabulate(outtable_cos, tablefmt=\"html\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
