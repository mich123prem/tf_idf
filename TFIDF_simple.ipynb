{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TFIDF and cosine similarity - toy example\n",
    "#### Inspired by, and partly taken from the contributions of <a href=\"https://markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/\">Mark Needham</a>  and <a href=\"https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089\">William Scott</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This Notebook demonstrate\n",
    "#### Preprocessing of texts for retrieval using the NLTK package\n",
    "\n",
    "#### the use of TFIDF in retrieval. <br>  ~6000 very short documents (stored in the papers/ directory) are read into memory, preprocessed to various degrees and are indexed for retrieval.<br> \n",
    "#### A toy of a toy (10 documents) are available in the directory <b>papers1/</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('papers1/822430.txt', 'Operating System Directions for the Next Millennium')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "import glob\n",
    "#\n",
    "corpus = [] # A list of tuples\n",
    "\n",
    "i=0\n",
    "for file in glob.glob(\"papers1/*.txt\"): #\"papers1/*.txt\" - 10 documents ...\n",
    "    with open(file, \"r\") as paper:\n",
    "#        filesfile.write(file[7:-4]+\":  \"+paper.read()+\"\\n\")\n",
    "        corpus.append((file, paper.read()))\n",
    "        i+=1\n",
    "\n",
    "#Define N, the number of documents\n",
    "N=len(corpus)\n",
    "print(corpus[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_split(doc_or_query):\n",
    "    tokens = doc_or_query.lower().split()\n",
    "    processed_doc_or_query = []\n",
    "    for w in tokens:\n",
    "\n",
    "      \n",
    "        if w not in stopwords.words(\"english\"):\n",
    "            processed_doc_or_query.append(w)\n",
    "    return processed_doc_or_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "## we introduce preprocessing in two steps that use the nltk-package to different degrees\n",
    "### 1. simple_preprocess() which only uses stop-words and punct. removal\n",
    "### 2. preprocess():             here we can comment in / out different steps, to see the effect\n",
    "### remember to call the correct function both for texts AND queries when experimenting with different preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the simple preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1. simple_preprocess:\n",
    "#### HERE WE ONLY IMPORT STOPWORDS LIST FROM NLTK, AND HANDLE PUNCTUATION\n",
    "\n",
    "#### The nltk-package has a lot of useful tools for language technology.<br> \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "# HERE WE USE THE STOPWORDS (NO Stemming, Lemmatization or any other stuff)\n",
    "def simple_preprocess(doc_or_query):\n",
    "    print(\"before:\",doc_or_query)\n",
    "    # returns a list of tokens\n",
    "    txt = doc_or_query\n",
    "\n",
    "    # REMOVE PUNCTUATION\n",
    "    for ch in symbols:\n",
    "        txt = txt.replace(ch, \" \")  # re.sub(string.punctuation, \" \", doc[1])\n",
    "    doc_or_query = txt\n",
    "    print(\"after:\",doc_or_query)\n",
    "    return token_split(txt)\n",
    "    # txt.lower() standardizes to low-case characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the more elaborate preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. preprocess:\n",
    "#### MORE ELABORATE PREPROCESSING WHERE STEPS CAN BE SWITCHED OUT\n",
    "#### BY COMMENTING OUT LINE\n",
    "\n",
    "import preprocess as pp  # We import the python file preprocess.py with preprocessing function\n",
    "\n",
    "\n",
    "def preprocess(doc_or_query):\n",
    "    print(\"before:\",doc_or_query)\n",
    "    doc_or_query = pp.convert_lower_case(doc_or_query)\n",
    "    \n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  # remove comma seperately\n",
    "    \n",
    "    doc_or_query = pp.remove_apostrophe(doc_or_query)\n",
    "    doc_or_query = pp.remove_stop_words(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(doc_or_query)\n",
    "    doc_or_query = pp.remove_punctuation(doc_or_query)\n",
    "    doc_or_query = pp.convert_numbers(doc_or_query)\n",
    "    doc_or_query = pp.stemming(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as we need to stem the words\n",
    "    doc_or_query = pp.remove_punctuation(\n",
    "        doc_or_query\n",
    "    )  \n",
    "    # needed again as num2word is giving few hypens and commas fourty-one\n",
    "    doc_or_query = pp.remove_stop_words(\n",
    "        doc_or_query\n",
    "    )\n",
    "    print(\"after:\",doc_or_query)\n",
    "\n",
    "    return token_split(doc_or_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Operating System Directions for the Next Millennium\n",
      "after:  oper system direct next millennium\n",
      "before: Operating System Concepts, 4th Ed.\n",
      "after:  oper system concept 4th ed\n",
      "before: On attaining reliable software for a secure operating system\n",
      "after:  attain reliabl softwar secur oper system\n",
      "before: Removing backing store administration from the CAP operating system\n",
      "after:  remov back store administr cap oper system\n",
      "before: Reflective program generation with patterns\n",
      "after:  reflect program gener pattern\n",
      "before: Can We Make Operating Systems Reliable and Secure?\n",
      "after:  make oper system reliabl secur\n",
      "before: Designing a global name service\n",
      "after:  design global name servic\n",
      "before: Adaptive feedback techniques for synchronized multimedia retrieval over integrated networks\n",
      "after:  adapt feedback techniqu synchron multimedia retriev integr network\n",
      "before: A hierarchical fair service curve algorithm for link-sharing, real-time and priority services\n",
      "after:  hierarch fair servic curv algorithm link share real time prioriti servic\n",
      "before: The Chubby lock service for loosely-coupled distributed systems\n",
      "after:  chubbi lock servic loo coupl distribut system\n",
      "ctr 10\n",
      "processed_corpus[0]= ['oper', 'system', 'direct', 'next', 'millennium']\n",
      "DF= {'oper': 5, 'system': 6, 'direct': 1, 'next': 1, 'millennium': 1, 'concept': 1, '4th': 1, 'ed': 1, 'attain': 1, 'reliabl': 2, 'softwar': 1, 'secur': 2, 'remov': 1, 'back': 1, 'store': 1, 'administr': 1, 'cap': 1, 'reflect': 1, 'program': 1, 'gener': 1, 'pattern': 1, 'make': 1, 'design': 1, 'global': 1, 'name': 1, 'servic': 3, 'adapt': 1, 'feedback': 1, 'techniqu': 1, 'synchron': 1, 'multimedia': 1, 'retriev': 1, 'integr': 1, 'network': 1, 'hierarch': 1, 'fair': 1, 'curv': 1, 'algorithm': 1, 'link': 1, 'share': 1, 'real': 1, 'time': 1, 'prioriti': 1, 'chubbi': 1, 'lock': 1, 'loo': 1, 'coupl': 1, 'distribut': 1}\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "### aDF calculated in advance\n",
    "symbols = r\"!\\\"#$%&()*+-—.,/:;<=>?@[\\]^_`{|}~\"\n",
    "  \n",
    "DF = dict() # dictionary \"Associative Array \"  \n",
    "c=0\n",
    "processed_corpus=[]#An array of token arrays\n",
    "ctr=0\n",
    "for doc in corpus:\n",
    "    processed_text=\"\"\n",
    "    txt=doc[1]\n",
    "    processed_tokens=preprocess(txt)\n",
    "    \n",
    "    #DF includes actually our vocabulary, and for each word its global weight \n",
    "    for w in processed_tokens:\n",
    "        try:\n",
    "            # DF[w] is a set, and each document will only be added once.\n",
    "            DF[w].add(ctr)\n",
    "        except:\n",
    "            DF[w] = {ctr}\n",
    "                \n",
    "    processed_corpus.append(processed_tokens)\n",
    "    ctr += 1\n",
    "print(\"ctr\",ctr)\n",
    "# At the end ctr = N\n",
    "\n",
    "# WE only need the number of distinct documents indexed  by each word.\n",
    "for j in DF:\n",
    "    DF[j]=len(DF[j])\n",
    "\n",
    "    #Print the first token array in processed_corpus\n",
    "print(\"processed_corpus[0]=\",processed_corpus[0])\n",
    "print(\"DF=\", DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Form the TFIDF valued TD-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max 1 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "max 2 <class 'int'>\n",
      "max 1 <class 'int'>\n",
      "{(0, 'direct'): 3.321928094887362, (0, 'millennium'): 3.321928094887362, (0, 'next'): 3.321928094887362, (0, 'oper'): 1.0, (0, 'system'): 0.7369655941662062, (1, '4th'): 3.321928094887362, (1, 'concept'): 3.321928094887362, (1, 'ed'): 3.321928094887362, (1, 'oper'): 1.0, (1, 'system'): 0.7369655941662062, (2, 'attain'): 3.321928094887362, (2, 'oper'): 1.0, (2, 'reliabl'): 2.321928094887362, (2, 'secur'): 2.321928094887362, (2, 'softwar'): 3.321928094887362, (2, 'system'): 0.7369655941662062, (3, 'administr'): 3.321928094887362, (3, 'back'): 3.321928094887362, (3, 'cap'): 3.321928094887362, (3, 'oper'): 1.0, (3, 'remov'): 3.321928094887362, (3, 'store'): 3.321928094887362, (3, 'system'): 0.7369655941662062, (4, 'gener'): 3.321928094887362, (4, 'pattern'): 3.321928094887362, (4, 'program'): 3.321928094887362, (4, 'reflect'): 3.321928094887362, (5, 'make'): 3.321928094887362, (5, 'oper'): 1.0, (5, 'reliabl'): 2.321928094887362, (5, 'secur'): 2.321928094887362, (5, 'system'): 0.7369655941662062, (6, 'design'): 3.321928094887362, (6, 'global'): 3.321928094887362, (6, 'name'): 3.321928094887362, (6, 'servic'): 1.736965594166206, (7, 'adapt'): 3.321928094887362, (7, 'feedback'): 3.321928094887362, (7, 'integr'): 3.321928094887362, (7, 'multimedia'): 3.321928094887362, (7, 'network'): 3.321928094887362, (7, 'retriev'): 3.321928094887362, (7, 'synchron'): 3.321928094887362, (7, 'techniqu'): 3.321928094887362, (8, 'algorithm'): 3.321928094887362, (8, 'curv'): 3.321928094887362, (8, 'fair'): 3.321928094887362, (8, 'hierarch'): 3.321928094887362, (8, 'link'): 3.321928094887362, (8, 'prioriti'): 3.321928094887362, (8, 'real'): 3.321928094887362, (8, 'servic'): 3.473931188332412, (8, 'share'): 3.321928094887362, (8, 'time'): 3.321928094887362, (9, 'chubbi'): 3.321928094887362, (9, 'coupl'): 3.321928094887362, (9, 'distribut'): 3.321928094887362, (9, 'lock'): 3.321928094887362, (9, 'loo'): 3.321928094887362, (9, 'servic'): 1.736965594166206, (9, 'system'): 0.7369655941662062}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "doc = 0\n",
    "tf_idf = {}  # Initializing the matrix\n",
    "showed = {}\n",
    "for d in range(N):  # For all documents\n",
    "    tokens = processed_corpus[d]\n",
    "    if len(tokens) == 0:\n",
    "        continue\n",
    "    counter = Counter(tokens)  # counts unique tokens in the tokens array\n",
    "    # and creates a dictionary of tokens counts\n",
    "    #print(\"counter\", counter)\n",
    "    maxi_fij=counter.most_common(1)[0][1]\n",
    "    print(\"max\",maxi_fij, type(maxi_fij))\n",
    "    \n",
    "    words_count = len(tokens)\n",
    "    for token in np.unique(tokens):  # sorted unique tokens\n",
    "        #\n",
    "        #== CAN CHANGE THE DEFINITION OF TF ==\n",
    "        #==     SEE Table 3.4 in book       == \n",
    "        #\n",
    "        tf=counter[token]\n",
    "        #tf = 0.5 + 0.5*(counter[token]/maxi_fij)# counter[token]  # /words_count\n",
    "        logtf = 1 + np.log2(tf)  # log\n",
    "        if token in DF:\n",
    "            df = DF[token]\n",
    "        \n",
    "            idf = np.log2( ((N)/ df))  # log\n",
    "\n",
    "            tf_idf[d, token] = (\n",
    "                tf * idf\n",
    "            )  # tf_idf is implemented as tuple-keyed dictionary\n",
    "        else: #Eq. 3.7\n",
    "            tf_idf[d, token]=0\n",
    "doc += 1\n",
    "# Printing an example from the Matrix: the TFIDF value of the word \"Automated\" in document 1\n",
    "print(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple matching of a query   \n",
    "## add up term - TFIDF scores for each doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "outtable_simple = []\n",
    "\n",
    "\n",
    "\n",
    "def matching_score(query):\n",
    "    # Process the query just like you processed the documents\n",
    "    # remove punctuation\n",
    "    processed_tokens = preprocess(query)\n",
    "\n",
    "    tokens = processed_tokens\n",
    "\n",
    "    query_weights = {}\n",
    "    # Simply add up the tfidfs for the words in the documents they index.\n",
    "    for key in tf_idf:\n",
    "        # remember, key is composed like this (key[0] = document_number, key[1] = token)\n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                print(\"key[1]\", key[1])\n",
    "                if key[0] not in query_weights:\n",
    "                    query_weights[key[0]]=0.0\n",
    "                query_weights[key[0]] += tf_idf[key]  # Accummulate and add tfidf-values for the term in each document\n",
    "            except:\n",
    "                print(\"exception:\", key, tf_idf[key])\n",
    "                query_weights[key[0]] = tf_idf[key]  #\n",
    "            # print(\"query_weights[\"+str(key[0])+\"]=\"+str(query_weights[key[0]]))\n",
    "    # Sort the resulting weights to give us a ranked list\n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    #print(\"query_weights=\", query_weights)\n",
    "    # print(\"query_weights\", query_weights)\n",
    "\n",
    "    l = []\n",
    "    qw = []\n",
    "    # List the first 10 matching documents\n",
    "    for i in query_weights[:10]:\n",
    "        l.append(i[0])\n",
    "        qw.append(i[1])\n",
    "    queue = deque(qw)\n",
    "    outtable_simple.append([\"Query: \", \"'\"+query+\"'\", \"\"])\n",
    "    outtable_simple.append([\"doc_nr\", \"doc\", \"score\"])\n",
    "    for d in l:\n",
    "        score = queue.popleft()\n",
    "        outtable_simple.append([d, corpus[d], score])\n",
    "        # print(d,corpus[d], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: systems\n",
      "after:  system\n",
      "key[1] system\n",
      "key[1] system\n",
      "key[1] system\n",
      "key[1] system\n",
      "key[1] system\n",
      "key[1] system\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Query:</td><td>'systems'                                                                                    </td><td>                  </td></tr>\n",
       "<tr><td>doc_nr</td><td>doc                                                                                          </td><td>score             </td></tr>\n",
       "<tr><td>0     </td><td>('papers1/822430.txt', 'Operating System Directions for the Next Millennium')                </td><td>0.7369655941662062</td></tr>\n",
       "<tr><td>1     </td><td>('papers1/562353.txt', 'Operating System Concepts, 4th Ed.')                                 </td><td>0.7369655941662062</td></tr>\n",
       "<tr><td>2     </td><td>('papers1/808449.txt', 'On attaining reliable software for a secure operating system')       </td><td>0.7369655941662062</td></tr>\n",
       "<tr><td>3     </td><td>('papers1/850712.txt', 'Removing backing store administration from the CAP operating system')</td><td>0.7369655941662062</td></tr>\n",
       "<tr><td>5     </td><td>('papers1/1137291.txt', 'Can We Make Operating Systems Reliable and Secure?')                </td><td>0.7369655941662062</td></tr>\n",
       "<tr><td>9     </td><td>('papers1/1298487.txt', 'The Chubby lock service for loosely-coupled distributed systems')   </td><td>0.7369655941662062</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "query=\"systems\"\n",
    "matching_score(query)\n",
    "\n",
    "display(HTML(tabulate.tabulate(outtable_simple, tablefmt='html')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
